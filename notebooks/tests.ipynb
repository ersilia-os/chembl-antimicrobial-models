{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deffcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flaml.default import RandomForestClassifier as ZeroShotRandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from flaml.automl import AutoML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Get all pathogens i.e. {pathogen}_{target}\n",
    "PATHOGENS = sorted(os.listdir(os.path.join(\"..\", \"data\")))\n",
    "\n",
    "# Define some paths\n",
    "PATH_TO_FEATURES = os.path.join(\"..\", \"output\", \"02_features\")\n",
    "PATH_TO_OUTPUT = os.path.join(\"..\", \"output\", \"03_baseline_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e430dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaiveBayesClassificationModel(X, Y, n_folds=5):\n",
    "\n",
    "    # Fit model with all data\n",
    "    model_all = MultinomialNB()\n",
    "    model_all.fit(X, Y)\n",
    "\n",
    "    # Cross-validations\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    aurocs = []\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        model_cv = MultinomialNB()\n",
    "        model_cv.fit(X_train, y_train)\n",
    "        fpr, tpr, _ = roc_curve(y_test, model_cv.predict_proba(X_test)[:, 1])\n",
    "        auroc = auc(fpr, tpr)\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    return model_all, [str(round(i, 4)) for i in aurocs]\n",
    "\n",
    "\n",
    "def RandomForestClassificationModel(X, Y, n_folds=5):\n",
    "\n",
    "    # Fit model with all data\n",
    "    zero_shot = ZeroShotRandomForestClassifier()\n",
    "    hyperparams = zero_shot.suggest_hyperparams(X, Y)[0]\n",
    "    hyperparams['n_jobs'] = 8\n",
    "    # hyperparams['n_estimators'] = 10\n",
    "    # print(\"Training full model...\")\n",
    "    model_all = RandomForestClassifier(**hyperparams)\n",
    "    model_all.fit(X, Y)\n",
    "\n",
    "    # Cross-validations\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    aurocs = []\n",
    "    for train_index, test_index in skf.split(X, Y):\n",
    "        # print(\"Training CV model...\")\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        zero_shot_cv = ZeroShotRandomForestClassifier()\n",
    "        hyperparams = zero_shot_cv.suggest_hyperparams(X_train, y_train)[0]\n",
    "        hyperparams['n_jobs'] = 8\n",
    "        # hyperparams['n_estimators'] = 10\n",
    "        model_cv = RandomForestClassifier(**hyperparams)\n",
    "        model_cv.fit(X_train, y_train)\n",
    "        fpr, tpr, _ = roc_curve(y_test, model_cv.predict_proba(X_test)[:, 1])\n",
    "        auroc = auc(fpr, tpr)\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    return model_all, [str(round(i, 4)) for i in aurocs]\n",
    "\n",
    "\n",
    "def FLAMLClassificationModel(X, Y, N_FOLDS=5, SPLITTING_ROUNDS=3):\n",
    "\n",
    "    # Constants for AutoML\n",
    "    N_FOLDS = 5 #5\n",
    "    SPLITTING_ROUNDS = 1 #3\n",
    "    FLAML_ESTIMATOR_LIST = [\"rf\"]\n",
    "    FLAML_COLD_MINIMUM_TIME_BUDGET_SECONDS = 250\n",
    "    FLAML_COLD_MAXIMUM_TIME_BUDGET_SECONDS = 500\n",
    "    FLAML_WARM_MINIMUM_TIME_BUDGET_SECONDS = 50\n",
    "    FLAML_WARM_MAXIMUM_TIME_BUDGET_SECONDS = 250\n",
    "    FLAML_COLD_MINIMUM_ITERATIONS = 50\n",
    "    FLAML_COLD_MAXIMUM_ITERATIONS = 250\n",
    "    FLAML_WARM_MINIMUM_ITERATIONS = 25\n",
    "    FLAML_WARM_MAXIMUM_ITERATIONS = 100\n",
    "\n",
    "    FLAML_COLD_MINIMUM_TIME_BUDGET_SECONDS = 50\n",
    "    FLAML_COLD_MAXIMUM_TIME_BUDGET_SECONDS = 100\n",
    "    FLAML_WARM_MINIMUM_TIME_BUDGET_SECONDS = 50\n",
    "    FLAML_WARM_MAXIMUM_TIME_BUDGET_SECONDS = 100\n",
    "\n",
    "    # Clean up log files function\n",
    "    def clean_log(log_file_name):\n",
    "        cwd = os.getcwd()\n",
    "        log_file = os.path.join(cwd, log_file_name)\n",
    "        if os.path.exists(log_file):\n",
    "            os.remove(log_file)\n",
    "\n",
    "    # Get AutoML settings based on data size\n",
    "    def get_automl_settings(time_budget, num_samples):\n",
    "        return {\n",
    "            \"time_budget\": int(\n",
    "                np.clip(\n",
    "                    time_budget,\n",
    "                    FLAML_COLD_MINIMUM_TIME_BUDGET_SECONDS,\n",
    "                    FLAML_COLD_MAXIMUM_TIME_BUDGET_SECONDS,\n",
    "                )\n",
    "            ),\n",
    "            \"metric\": \"roc_auc\",\n",
    "            \"task\": \"classification\",\n",
    "            \"log_file_name\": \"flaml.log\",\n",
    "            \"log_training_metric\": True,\n",
    "            \"verbose\": 0,\n",
    "            \"n_jobs\": 8,\n",
    "            \"early_stop\": True,\n",
    "            \"max_iter\": int(\n",
    "                np.clip(\n",
    "                    num_samples / 3,\n",
    "                    FLAML_COLD_MINIMUM_ITERATIONS,\n",
    "                    FLAML_COLD_MAXIMUM_ITERATIONS,\n",
    "                )\n",
    "            ),\n",
    "            \"estimator_list\": FLAML_ESTIMATOR_LIST\n",
    "        }\n",
    "    \n",
    "    # Extract best configuration from a model\n",
    "    def get_starting_point(model):\n",
    "        best_models = model.best_config_per_estimator\n",
    "        best_estimator = model.best_estimator\n",
    "        return {best_estimator: best_models[best_estimator]}\n",
    "\n",
    "    # Initial \"cold\" fitting phase - broad search\n",
    "    def fit_cold(X, y):\n",
    "        # print(\"Starting cold fitting phase...\")\n",
    "        automl_settings = get_automl_settings(\n",
    "            time_budget=(FLAML_COLD_MINIMUM_TIME_BUDGET_SECONDS + FLAML_COLD_MAXIMUM_TIME_BUDGET_SECONDS) / 2,\n",
    "            num_samples=len(y)\n",
    "        )\n",
    "        model = AutoML(n_jobs=8)\n",
    "        model.fit(\n",
    "            X_train=X, \n",
    "            y_train=y, \n",
    "            eval_method=\"auto\",\n",
    "            split_type=None,\n",
    "            groups=None,\n",
    "            **automl_settings\n",
    "        )\n",
    "        # print(f\"Cold fitting complete. Best estimator: {model.best_estimator}\")\n",
    "        \n",
    "        # Proceed to warm fitting\n",
    "        warm_model = fit_warm(X, y, model, automl_settings)\n",
    "        clean_log(automl_settings[\"log_file_name\"])\n",
    "        return warm_model, automl_settings\n",
    "\n",
    "     # Refined \"warm\" fitting phase - focus on best model\n",
    "    \n",
    "    # Refined \"warm\" fitting phase - focus on best model\n",
    "    def fit_warm(X, y, cold_model, automl_settings):\n",
    "        # print(\"Starting warm fitting phase...\")\n",
    "        warm_settings = automl_settings.copy()\n",
    "        warm_settings[\"time_budget\"] = int(\n",
    "            np.clip(\n",
    "                warm_settings[\"time_budget\"] * 0.5,\n",
    "                FLAML_WARM_MINIMUM_TIME_BUDGET_SECONDS,\n",
    "                FLAML_WARM_MAXIMUM_TIME_BUDGET_SECONDS,\n",
    "            )\n",
    "        )\n",
    "        warm_settings[\"log_file_name\"] = \"warm_flaml.log\"\n",
    "        warm_settings[\"estimator_list\"] = [cold_model.best_estimator]\n",
    "        warm_settings[\"max_iter\"] = int(\n",
    "            np.clip(\n",
    "                int(warm_settings[\"max_iter\"] * 0.5) + 1,\n",
    "                FLAML_WARM_MINIMUM_ITERATIONS,\n",
    "                FLAML_WARM_MAXIMUM_ITERATIONS,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        starting_point = get_starting_point(cold_model)\n",
    "        model = AutoML(n_jobs=8)\n",
    "        model.fit(\n",
    "            X_train=X, \n",
    "            y_train=y, \n",
    "            starting_points=starting_point,\n",
    "            eval_method=\"auto\",\n",
    "            split_type=None, \n",
    "            groups=None,\n",
    "            **warm_settings\n",
    "        )\n",
    "        clean_log(warm_settings[\"log_file_name\"])\n",
    "        # print(\"Warm fitting complete.\")\n",
    "        return model\n",
    "\n",
    "    # Cross-validation prediction to get out-of-sample estimates\n",
    "    def fit_predict_cv(X, y, model, automl_settings):\n",
    "        # print(f\"Performing {SPLITTING_ROUNDS} rounds of {N_FOLDS}-fold cross-validation...\")\n",
    "        cv_settings = automl_settings.copy()\n",
    "        cv_settings[\"time_budget\"] = int(\n",
    "            np.clip(\n",
    "                cv_settings[\"time_budget\"] * 0.5,\n",
    "                FLAML_WARM_MINIMUM_TIME_BUDGET_SECONDS,\n",
    "                FLAML_WARM_MAXIMUM_TIME_BUDGET_SECONDS,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        best_estimator = model.best_estimator\n",
    "        starting_point = get_starting_point(model)\n",
    "        \n",
    "        # Create a dictionary to store results for each sample\n",
    "        results = collections.defaultdict(list)\n",
    "        aurocs = []\n",
    "        \n",
    "        # Create stratified k-fold splitter\n",
    "        splitter = StratifiedKFold(n_splits=N_FOLDS, shuffle=True)\n",
    "        \n",
    "        # Loop through multiple splitting rounds\n",
    "        k = 0\n",
    "        for _ in range(SPLITTING_ROUNDS):\n",
    "            for train_index, test_index in splitter.split(X, y):\n",
    "                # print(f\"CV split {k+1}/{N_FOLDS*SPLITTING_ROUNDS}\")\n",
    "                \n",
    "                # Extract train and test data\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                \n",
    "                # Configure settings for this fold\n",
    "                fold_settings = cv_settings.copy()\n",
    "                fold_settings[\"log_file_name\"] = f\"oos_{k}_automl.log\"\n",
    "                fold_settings[\"estimator_list\"] = [best_estimator]\n",
    "                fold_settings[\"max_iter\"] = int(\n",
    "                    np.clip(\n",
    "                        int(fold_settings[\"max_iter\"] * 0.25) + 1,\n",
    "                        FLAML_WARM_MINIMUM_ITERATIONS,\n",
    "                        FLAML_WARM_MAXIMUM_ITERATIONS,\n",
    "                    )\n",
    "                )\n",
    "                # Train a model on this fold\n",
    "                fold_model = AutoML(n_jobs=8)\n",
    "                fold_model.fit(\n",
    "                    X_train=X_train,\n",
    "                    y_train=y_train,\n",
    "                    starting_points=starting_point,\n",
    "                    eval_method=\"auto\",\n",
    "                    split_type=None,\n",
    "                    groups=None,\n",
    "                    **fold_settings\n",
    "                )\n",
    "                \n",
    "                # Get the base estimator\n",
    "                base_model = fold_model.model.estimator\n",
    "                \n",
    "                # Try to calibrate the model for better probability estimates\n",
    "                # print(\"Fitting a calibrated model for better probability estimates\")\n",
    "                try:\n",
    "                    calibrated_model = CalibratedClassifierCV(estimator=base_model, n_jobs=8)\n",
    "                    calibrated_model.fit(X_train, y_train, n_jobs=8)\n",
    "                    fold_estimator = calibrated_model\n",
    "                except Exception as e:\n",
    "                    # print(f\"Could not calibrate model: {e}. Continuing with uncalibrated model...\")\n",
    "                    fold_estimator = base_model\n",
    "                \n",
    "                # Get probabilistic predictions for test data\n",
    "                y_pred_proba = fold_estimator.predict_proba(X_test)[:, 1]\n",
    "                \n",
    "                # # Store predictions for each test sample\n",
    "                # for i, idx in enumerate(test_index):\n",
    "                #     results[idx].append(y_pred_proba[i])\n",
    "                aurocs.append(roc_auc_score(y_test, y_pred_proba))\n",
    "                \n",
    "                # Clean up logs\n",
    "                clean_log(fold_settings[\"log_file_name\"])\n",
    "                k += 1\n",
    "        \n",
    "        # # Average predictions for each sample\n",
    "        # y_hat = []\n",
    "        # for i in range(len(y)):\n",
    "        #     y_hat.append(np.mean(results[i]))\n",
    "        \n",
    "        # return {\"y_hat\": np.array(y_hat), \"y\": y}\n",
    "        # return results\n",
    "        return aurocs\n",
    "\n",
    "    # ---- Main Training Process ----\n",
    "    \n",
    "    # 1. Train cold model (which gets refined with warm fitting)\n",
    "    model, automl_settings = fit_cold(X, Y)\n",
    "    \n",
    "    # 2. Perform cross-validation to get out-of-sample predictions\n",
    "    cv_results = fit_predict_cv(X, Y, model, automl_settings)\n",
    "    \n",
    "    # 3. Train final model on all data\n",
    "    # print(\"Training final calibrated model on all data...\")\n",
    "    try:\n",
    "        final_model = CalibratedClassifierCV(model.model.estimator, n_jobs=8)\n",
    "        final_model.fit(X, Y, n_jobs=8)\n",
    "    except Exception as e:\n",
    "        # print(f\"Could not calibrate final model: {e}. Using uncalibrated model...\")\n",
    "        final_model = model.model.estimator\n",
    "\n",
    "    return final_model, cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- PATHOGEN: abaumannii_organism ---------------------------\n",
      "TASK: 1_assay_CHEMBL4296188_Inhibition_percentage_activity_percentile_10_ORGANISM_1.csv\n",
      "Training NB...\n",
      "Training RF...\n"
     ]
    }
   ],
   "source": [
    "for pathogen in PATHOGENS:\n",
    "\n",
    "    print(f\"----------------------- PATHOGEN: {pathogen} ---------------------------\")\n",
    "\n",
    "    # Get list of tasks\n",
    "    tasks = sorted(os.listdir(os.path.join(\"..\", \"data\", pathogen)))\n",
    "\n",
    "    # Get IK to MFP\n",
    "    IKs = open(os.path.join(PATH_TO_FEATURES, pathogen, 'IKS.txt')).read().splitlines()\n",
    "    MFPs = np.load(os.path.join(PATH_TO_FEATURES, pathogen, \"X.npz\"))['X']\n",
    "    IK_TO_MFP = {i: j for i, j in zip(IKs, MFPs)}\n",
    "\n",
    "    # For each task\n",
    "    for task in tasks:\n",
    "\n",
    "        print(f\"TASK: {task}\")\n",
    "\n",
    "        # Create output_dir\n",
    "        output_dir = os.path.join(PATH_TO_OUTPUT, pathogen, task.replace(\".csv\", \"\"))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Load data\n",
    "        df = pd.read_csv(os.path.join(\"..\", \"data\", pathogen, task))\n",
    "        cols = df.columns.tolist()\n",
    "        X, Y = [], []\n",
    "        for ik, act in zip(df['inchikey'], df[cols[2]]):\n",
    "            if ik in IK_TO_MFP:\n",
    "                X.append(IK_TO_MFP[ik])\n",
    "                Y.append(act)\n",
    "\n",
    "        # To np.array\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        print(\"Training NB...\")\n",
    "\n",
    "        # Naive Bayes\n",
    "        NB, results_NB = NaiveBayesClassificationModel(X, Y)\n",
    "        joblib.dump(NB, os.path.join(output_dir, \"NB.joblib\"))\n",
    "        with open(os.path.join(output_dir, \"NB_CV.csv\"), \"w\") as f:\n",
    "            f.write(\",\".join(results_NB))\n",
    "\n",
    "        print(\"Training RF...\")\n",
    "\n",
    "        # Random Forest\n",
    "        RF, results_RF = RandomForestClassificationModel(X, Y)\n",
    "        joblib.dump(RF, os.path.join(output_dir, \"RF.joblib\"))\n",
    "        with open(os.path.join(output_dir, \"RF_CV.csv\"), \"w\") as f:\n",
    "            f.write(\",\".join(results_RF))\n",
    "\n",
    "        print(\"Training FLAML...\")\n",
    "\n",
    "        # FLAML\n",
    "        FLAML, results_FLAML = FLAMLClassificationModel(X, Y)\n",
    "        joblib.dump(FLAML, os.path.join(output_dir, \"FLAML.joblib\"))\n",
    "        with open(os.path.join(output_dir, \"FLAML_CV.csv\"), \"w\") as f:\n",
    "            f.write(\",\".join(results_FLAML))\n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7293ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d073d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "camm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
